https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/
Also: see http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/


What model is used after calculations?

Adaptive boosting:
AdaBoost works by weighting the observations, putting more weight on difficult to classify instances
and less on those already handled well. 
New weak learners are added sequentially that focus their training on the more difficult patterns.
Predictions are made by majority vote of the weak learners’ predictions, weighted by their individual accuracy

How Gradient Boosting Works
Gradient boosting involves three elements:

A loss function to be optimized.
A weak learner to make predictions.
An additive model to add weak learners to minimize the loss function.

Additive Model
Trees are added one at a time, and existing trees in the model are not changed.
A gradient descent procedure is used to minimize the loss when adding trees.
Traditionally, gradient descent is used to minimize a set of parameters, such as the coefficients in a regression equation
or weights in a neural network. After calculating error or loss, the weights are updated to minimize that error.

Instead of parameters, we have weak learner sub-models or more specifically decision trees. 
After calculating the loss, to perform the gradient descent procedure, we must add a tree to the model that reduces the loss
(i.e. follow the gradient). We do this by parameterizing the tree, then modify the parameters of the tree and move
in the right direction by (reducing the residual loss.
Generally this approach is called functional gradient descent or gradient descent with functions.

One way to produce a weighted combination of classifiers which optimizes [the cost] is by gradient descent in function space
— Boosting Algorithms as Gradient Descent in Function Space [PDF], 1999

The output for the new tree is then added to the output of the existing sequence of trees in an effort
to correct or improve the final output of the model.
A fixed number of trees are added or training stops once loss reaches an acceptable level or no longer
improves on an external validation dataset.

Improvements to Basic Gradient Boosting
-Tree Constraints
-Shrinkage
-Random sampling
-Penalized Learning
